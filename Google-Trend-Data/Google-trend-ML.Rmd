---
title: "US inflation forecasting with ML models (v.s AR model)"
author: "Ziqi Zang"
date: "2/28/2019"
output: pdf_document
params:
  data: data(iclaims)
---
### Set global and markdown global options

```{r global.options, include = FALSE }
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    fig.width   = 6,       # the width for plots created by code chunk
    fig.height  = 4,       # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = TRUE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This example code compares an AR model, Adaptive Lasso and Adaptive Elastic Net forecasting abilities for the U.S inflation rate. Data stationarity is required for the models. 


Import the libraries 

```{r libraries, include=FALSE}
library(lubridate)
library(bsts)
library(ggplot2)
library(reshape2)
library(Boom)
library(graphics)
library(tibble)
library(readr)
library(magrittr)
library(plyr)
```

1. Construct the dataset for US inflation rate. 
```{r data_inf, include = FALSE}
data_dir <- "/Users/ziqizang/Documents/JMP/US_INF"
us_inf <- file.path(data_dir, "US_inf.csv")
us_inf_cor <- file.path(data_dir, "US_Inflation_Panel.csv")
us_inf <-as.data.frame(read.csv(us_inf, header = T))
us_inf_cor <-as.data.frame(read.csv(us_inf_cor, header = T,encoding = "UTF-8"))

us_inf_all = join_all(list(us_inf, us_inf_cor), by='Date', type='left')

## replace all zeros with ones to avoid an infinity value if taking log
us_inf_all[us_inf_all == 0] <- 1

## replace all character with NAs and then with ones
for (i in 2:ncol(us_inf_all)) {
us_inf_all[,i] = as.numeric(as.character(us_inf_all[,i]))
}
us_inf_all[is.na(us_inf_all)] <- 1
## construct a zoo object
us_inf_all<-read.zoo(us_inf_all, index = c("Date"), format = "%m/%d/%y")

## take diff and log of original dataset 
for (i in 2:ncol(us_inf_all)) {
us_inf_all[,i] = log(us_inf_all[,i])
}                         
us_inf_all[,1] = c(0,diff(us_inf_all[,1],1))

us_inf_all$inf_lag = lag(us_inf_all$inf,k = -1)
us_inf_all<-us_inf_all[2:(nrow(us_inf_all)),]

us_inf_all = na.omit(us_inf_all)
us_inf_all<-us_inf_all[2:(nrow(us_inf_all)),]

# us_inf_all$Date <- as.Date(us_inf_all$Date , "%m/%d/%y")

```

```{r}
library(dplyr)
library(ggplot2)
library(magrittr)  # getting %>% operator for dplyr
library(readr)
library(GGally)
library(gridExtra)  # layouts for ggplot
library(grid)  # layouts for ggplot

# best subset selection, forward and backward search
library(leaps)

# regularized regression
library(glmnet)

```

Adaptive LASSO for inflation
```{r}
#
library(rRAP)
library(lars)
library(forecast)
library(tidyverse)
library(magrittr)
library(glmnet)
library(pROC)
```


```{r}
Data = us_inf_all
X = matrix(Data[,-1 ],nrow = dim(Data[,-1 ])[1])
Y = matrix(Data[,1],nrow = dim(Data[,-1 ])[1])


j= 48
w_size = j
n_windows = nrow(us_inf_all) - j 
# initialize the coeff_prior and # of variable inclusiong for model updating... 
cov_df <- c()

df <- data.frame(matrix(ncol = 2, nrow = 0))
col_names <- c("coeff_name", "beta")
colnames(df) <- col_names

for (i in 1:n_windows) {

ridge1 <- glmnet(x = X[1:(w_size + i - 1),], y = Y[1:(w_size + i - 1),] ,
                 ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
                 alpha = 0)
# ridge1_cv <- cv.glmnet(x = X, y = Y, 
#                        ## type.measure: loss to use for cross-validation.
#                        type.measure = "mse",
#                        ## K = 10 is the default.
#                        nfold = 10,
#                        ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
#                        alpha = 0)


best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]

alasso1 <- glmnet(x = X[1:(w_size + i - 1),], y = Y[1:(w_size + i - 1),],
                  ## type.measure: loss to use for cross-validation.
                  ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
                  alpha = 1,
                  ##
                  ## penalty.factor: Separate penalty factors can be applied to each
                  ##           coefficient. This is a number that multiplies ‘lambda’ to
                  ##           allow differential shrinkage. Can be 0 for some variables,
                  ##           which implies no shrinkage, and that variable is always
                  ##           included in the model. Default is 1 for all variables (and
                  ##           implicitly infinity for variables listed in ‘exclude’). Note:
                  ##           the penalty factors are internally rescaled to sum to nvars,
                  ##           and the lambda sequence will reflect this change.
                  penalty.factor = 1 / abs(best_ridge_coef))

alasso1_cv <- cv.glmnet(x = X[1:(w_size + i - 1),], y = Y[1:(w_size + i - 1),],
                        ## type.measure: loss to use for cross-validation.
                        type.measure = "mse",
                        ## K = 10 is the default.
                        nfold = 10,
                        ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
                        alpha = 1,
                        ##
                        ## penalty.factor: Separate penalty factors can be applied to each
                        ##           coefficient. This is a number that multiplies ‘lambda’ to
                        ##           allow differential shrinkage. Can be 0 for some variables,
                        ##           which implies no shrinkage, and that variable is always
                        ##           included in the model. Default is 1 for all variables (and
                        ##           implicitly infinity for variables listed in ‘exclude’). Note:
                        ##           the penalty factors are internally rescaled to sum to nvars,
                        ##           and the lambda sequence will reflect this change.
                        penalty.factor = 1 / abs(best_ridge_coef),
                        ## prevalidated array is returned
                        keep = TRUE)
## Penalty vs CV MSE plot
# plot(alasso1_cv)
best_alasso_coef1 <- coef(alasso1_cv, s = alasso1_cv$lambda.min)
best_alasso_coef1 = as.numeric(best_alasso_coef1) 
cols = colnames(Data[,-1])

nonzerocoefficients <- data.frame(cols,best_alasso_coef1[-1] )
colnames(nonzerocoefficients) <- col_names
nonzerocoefficients <- subset(nonzerocoefficients, beta!=0)
number_cov = dim(nonzerocoefficients)[1]
cov_df <-rbind(cov_df, number_cov)
df <- rbind(df, nonzerocoefficients)


}
```

Adaptive Elastic Net Model +  Pre -SIS procedure 

```{r}
library(glm2)
library(SISadaElasticNet) # local R package https://github.com/zikiki/SISadaElasticNet
Data = us_inf_all

X = matrix(Data[,-1 ],nrow = dim(Data[,-1 ])[1])
Y = matrix(Data[,1],nrow = dim(Data[,-1 ])[1])

j= 48
w_size = j
n_windows = nrow(us_inf_all) - j 
# initialize the coeff_prior and # of variable inclusiong for model updating... 
predicts_arnet<-c()
trues_arnet<-c()
predicts_anet<-c()
trues_anet<-c()
errors <-c()
for (i in 1:n_windows) {

Xsis<-SIS.gaussian(X[i:(w_size + i - 1),], matrix(Y[i:(w_size + i - 1),1]),120)
aenet.fit = aenet(Xsis$Xs, matrix(Y[i:(w_size + i - 1),1]),
alphas = seq(0.2, 0.8, 0.02), seed = 10)
coef(aenet.fit) ## return the regressors' coefficients (the order of the regressors are ranked from the SIS.gaussian function)
nonzero_index = which(!coef(aenet.fit) == 0) ## return the index of the regressors in the ordered list

# Build an AR model that includes these regressors as well as the lagged term
if (length(nonzero_index) !=0){
  nonzero_index <-as.vector(nonzero_index)
  xregs <- X[i:(w_size + i - 1),Xsis$Index[1:120][nonzero_index]]
  newxregs <-t(matrix(X[w_size + i,Xsis$Index[1:120][nonzero_index]]))
  
  Data_uni = ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12)
  AR_net <-Arima(Data_uni[i:(w_size + i - 1)], order=c(1,0,0),xreg =xregs)
  AR_net_predict <- forecast(AR_net, xreg = newxregs , h =1)
  y_real =Data_uni[w_size + i]
  
  predicts_arnet <- c(predicts_arnet, AR_net_predict$mean)
  trues_arnet <- c(trues_arnet, y_real)
  e<-y_real -  AR_net_predict$mean
} else {
  Data_uni = ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12)
  AR_net <-Arima(Data_uni[i:(w_size + i - 1)], order=c(1,0,0))
  AR_net_predict <- forecast(AR_net, h =1)
  y_real =Data_uni[w_size + i]
  
  predicts_arnet <- c(predicts_arnet, AR_net_predict$mean)
  trues_arnet <- c(trues_arnet, y_real)
  e<-y_real -  AR_net_predict$mean
}



## Only with adaptive elastic net model
#y_predict = predict(aenet.fit, t(matrix(X[w_size + i,Xsis$Index[1:120]])))
#y_real = matrix(Y[w_size + i])

#predicts_anet <- c(predicts_anet,y_predict)
#trues_anet <-c(trues_anet, y_real)
#ee<-y_real - y_predict
errors <-c(errors,e)
}

```


Benchmark model: AR model 
```{r}
#index(us_inf_all)

pacf(ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12))
Data = ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12)
##AR(1) Mode 


predicts_ar<-c()
trues_ar<-c()
errors_ar <-c()
for (i in 1:n_windows) {
  
AR_model <- arima(Data[i:(w_size + i - 1)], order=c(1,0,0))
AR_predict <- predict(AR_model, 1)
y_real =Data[w_size + i]

predicts_ar <- c(predicts_ar, AR_predict$pred)
trues_ar <- c(trues_ar, y_real)
e<-y_real -  AR_predict$pred

}

#ts.plot(Data[1:48], AR_predict$pred, lty = c(1,3), col=c(5,2))

```


```{r}
## plot the predicts and true 

values = seq(from = as.Date("2008-03-01"), to = as.Date("2019-01-01"), by = 'month')
df <- data.frame(date = values, predicts = predicts_arnet, true = trues_arnet)
colnames(df) <-c('date','Adaptive Elastic Net Out-of-sample Forecasts', 'Inflation Changes (US)')

df <- melt(df ,  id.vars = 'date')
ggplot(data = df) + geom_line(aes(x = date, y = value, linetype = variable, color = variable))
```
```{r}
## plot the predicts_ar and true 

values = seq(from = as.Date("2008-03-01"), to = as.Date("2019-01-01"), by = 'month')
df <- data.frame(date = values, predicts = predicts_ar, true = trues_ar)
colnames(df) <-c('date','AR Model Out-of-sample Forecasts', 'Inflation Changes (US)')

df <- melt(df ,  id.vars = 'date')
ggplot(data = df) + geom_line(aes(x = date, y = value, linetype = variable, color = variable))
```

compare LASSO and AR
```{r}


  mspe = trues - predicts

  mspe_L = mean(na.omit(mspe)^2)
  mspe_AR = mean(na.omit(mspe_ar)^2)
  mspe_ratio = mspe_L/mspe_AR

  dm_re = dm.test(mspe_ar, mspe, "greater",h = 1)
  
```

compare Lasso and RW

```{r}
  mspe_random = trues - tail(Data[,'inf_lag'], n=n_windows)

  mspe = trues - predicts

  mspe_L = mean(na.omit(mspe)^2)
  mspe_RW = mean(na.omit(mspe_random)^2)
  mspe_ratio_LR = mspe_L/mspe_RW

  
  
  dm_LR = dm.test(as.matrix(mspe_random), mspe, "greater",h = 1)
  
```


