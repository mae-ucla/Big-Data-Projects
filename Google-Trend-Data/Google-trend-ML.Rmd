---
title: "US inflation forecasting with ML models (v.s AR model)"
author: "Ziqi Zang"
date: "2/28/2019"
output: pdf_document
params:
  data: data(iclaims)
---
### Set global and markdown global options

```{r global.options, include = FALSE }
knitr::opts_chunk$set(
    cache       = TRUE,     # if TRUE knitr will cache the results to reuse in future knits
    fig.width   = 6,       # the width for plots created by code chunk
    fig.height  = 4,       # the height for plots created by code chunk
    fig.align   = 'center', # how to align graphics in the final doc. 'left', 'right', 'center'
    fig.path    = 'figs/',  # file path to the directory where knitr shall store the graphics files
    results     = 'asis',   # knitr will pass through results without reformatting them
    echo        = TRUE,     # in FALSE knitr will not display code in the code chunk above it's results
    message     = TRUE,     # if FALSE knitr will not display any messages generated by code
    strip.white = TRUE,     # if FALSE knitr will not remove white spaces at the beg or end of code chunk
    warning     = FALSE)    # if FALSE knitr will not display any warning messages in the final document
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This example code compares an AR model, Adaptive Lasso and Adaptive Elastic Net forecasting abilities for the U.S inflation rate. Data stationarity is required for the models. 


Import the libraries 

```{r libraries, include=FALSE}
library(lubridate)
library(bsts)
library(ggplot2)
library(reshape2)
library(Boom)
library(graphics)
library(tibble)
library(readr)
library(magrittr)
library(plyr)
```

1. Construct the dataset for US inflation rate. 
```{r data_inf, include = FALSE}
data_dir <- "/Users/ziqizang/Documents/JMP/US_INF"
us_inf <- file.path(data_dir, "US_inf.csv")
us_inf_cor <- file.path(data_dir, "US_Inflation_Panel.csv")
us_inf <-as.data.frame(read.csv(us_inf, header = T))
us_inf_cor <-as.data.frame(read.csv(us_inf_cor, header = T,encoding = "UTF-8"))

us_inf_all = join_all(list(us_inf, us_inf_cor), by='Date', type='left')

## replace all zeros with ones to avoid an infinity value if taking log
us_inf_all[us_inf_all == 0] <- 1

## replace all character with NAs and then with ones
for (i in 2:ncol(us_inf_all)) {
us_inf_all[,i] = as.numeric(as.character(us_inf_all[,i]))
}
us_inf_all[is.na(us_inf_all)] <- 1
## construct a zoo object
us_inf_all<-read.zoo(us_inf_all, index = c("Date"), format = "%m/%d/%y")

## take diff and log of original dataset 
for (i in 2:ncol(us_inf_all)) {
us_inf_all[,i] = c(0,diff(log(us_inf_all[,i]),1))
}                         

us_inf_all[,1] = c(0,diff(us_inf_all[,1],1))

us_inf_all$inf_lag = lag(us_inf_all$inf,k = -1)
us_inf_all<-us_inf_all[2:(nrow(us_inf_all)),]

us_inf_all = na.omit(us_inf_all)
us_inf_all<-us_inf_all[2:(nrow(us_inf_all)),]

# us_inf_all$Date <- as.Date(us_inf_all$Date , "%m/%d/%y")

```

```{r}
library(dplyr)
library(ggplot2)
library(magrittr)  # getting %>% operator for dplyr
library(readr)
library(GGally)
library(gridExtra)  # layouts for ggplot
library(grid)  # layouts for ggplot

# best subset selection, forward and backward search
library(leaps)

# regularized regression
library(glmnet)

```

Adaptive LASSO for inflation
```{r}
#
library(rRAP)
library(lars)
library(forecast)
library(tidyverse)
library(magrittr)
library(glmnet)
library(pROC)
```


```{r}
Data = us_inf_all
X = matrix(Data[,-1 ],nrow = dim(Data[,-1 ])[1])
Y = matrix(Data[,1],nrow = dim(Data[,-1 ])[1])


j= 48
w_size = j
n_windows = nrow(us_inf_all) - j 
# initialize the coeff_prior and # of variable inclusiong for model updating... 
cov_df <- c()

df <- data.frame(matrix(ncol = 2, nrow = 0))
col_names <- c("coeff_name", "beta")
colnames(df) <- col_names

for (i in 1:n_windows) {

ridge1 <- glmnet(x = X[1:(w_size + i - 1),], y = Y[1:(w_size + i - 1),] ,
                 ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
                 alpha = 0)
# ridge1_cv <- cv.glmnet(x = X, y = Y, 
#                        ## type.measure: loss to use for cross-validation.
#                        type.measure = "mse",
#                        ## K = 10 is the default.
#                        nfold = 10,
#                        ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
#                        alpha = 0)


best_ridge_coef <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.1se))[-1]

alasso1 <- glmnet(x = X[1:(w_size + i - 1),], y = Y[1:(w_size + i - 1),],
                  ## type.measure: loss to use for cross-validation.
                  ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
                  alpha = 1,
                  ##
                  ## penalty.factor: Separate penalty factors can be applied to each
                  ##           coefficient. This is a number that multiplies ‘lambda’ to
                  ##           allow differential shrinkage. Can be 0 for some variables,
                  ##           which implies no shrinkage, and that variable is always
                  ##           included in the model. Default is 1 for all variables (and
                  ##           implicitly infinity for variables listed in ‘exclude’). Note:
                  ##           the penalty factors are internally rescaled to sum to nvars,
                  ##           and the lambda sequence will reflect this change.
                  penalty.factor = 1 / abs(best_ridge_coef))

alasso1_cv <- cv.glmnet(x = X[1:(w_size + i - 1),], y = Y[1:(w_size + i - 1),],
                        ## type.measure: loss to use for cross-validation.
                        type.measure = "mse",
                        ## K = 10 is the default.
                        nfold = 10,
                        ## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
                        alpha = 1,
                        ##
                        ## penalty.factor: Separate penalty factors can be applied to each
                        ##           coefficient. This is a number that multiplies ‘lambda’ to
                        ##           allow differential shrinkage. Can be 0 for some variables,
                        ##           which implies no shrinkage, and that variable is always
                        ##           included in the model. Default is 1 for all variables (and
                        ##           implicitly infinity for variables listed in ‘exclude’). Note:
                        ##           the penalty factors are internally rescaled to sum to nvars,
                        ##           and the lambda sequence will reflect this change.
                        penalty.factor = 1 / abs(best_ridge_coef),
                        ## prevalidated array is returned
                        keep = TRUE)
## Penalty vs CV MSE plot
# plot(alasso1_cv)
best_alasso_coef1 <- coef(alasso1_cv, s = alasso1_cv$lambda.min)
best_alasso_coef1 = as.numeric(best_alasso_coef1) 
cols = colnames(Data[,-1])

nonzerocoefficients <- data.frame(cols,best_alasso_coef1[-1] )
colnames(nonzerocoefficients) <- col_names
nonzerocoefficients <- subset(nonzerocoefficients, beta!=0)
number_cov = dim(nonzerocoefficients)[1]
cov_df <-rbind(cov_df, number_cov)
df <- rbind(df, nonzerocoefficients)


}
```

1. Adaptive Elastic Net Model +  Pre -SIS procedure  (Rolling Window)

```{r}
library(glm2)
library('SISadaElasticNet',lib.loc = "SISadaElasticNet")
library( msaenet)
        
 # local R package https://github.com/zikiki/SISadaElasticNet
Data = us_inf_all

X = matrix(Data[,-1 ],nrow = dim(Data[,-1 ])[1])
Y = matrix(Data[,1],nrow = dim(Data[,-1 ])[1])

j= 48
w_size = j
n_windows = nrow(us_inf_all) - j 
# initialize the coeff_prior and # of variable inclusiong for model updating... 
predicts_arnet<-c()
trues_arnet<-c()
predicts_anet<-c()
trues_anet<-c()
errors <-c()
for (i in 1:n_windows) {

Xsis<-SIS.gaussian(X[i:(w_size + i - 1),], matrix(Y[i:(w_size + i - 1),1]),48)
aenet.fit = aenet(Xsis$Xs, matrix(Y[i:(w_size + i - 1),1]),
alphas = seq(0.2, 0.8, 0.02), seed = 10)
coef(aenet.fit) ## return the regressors' coefficients (the order of the regressors are ranked from the SIS.gaussian function)
nonzero_index = which(!coef(aenet.fit) == 0) ## return the index of the regressors in the ordered list

# Build an AR model that includes these regressors as well as the lagged term
if (length(nonzero_index) !=0){
  nonzero_index <-as.vector(nonzero_index)
  xregs <- X[i:(w_size + i - 1),Xsis$Index[1:48][nonzero_index]]
  newxregs <-t(matrix(X[w_size + i,Xsis$Index[1:48][nonzero_index]]))
  
  Data_uni = ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12)
  AR_net <-Arima(Data_uni[i:(w_size + i - 1)], order=c(1,0,0),xreg =xregs)
  AR_net_predict <- forecast(AR_net, xreg = newxregs , h =1)
  y_real =Data_uni[w_size + i]
  
  predicts_arnet <- c(predicts_arnet, AR_net_predict$mean)
  trues_arnet <- c(trues_arnet, y_real)
  e<-y_real -  AR_net_predict$mean
} else {
  Data_uni = ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12)
  AR_net <-Arima(Data_uni[i:(w_size + i - 1)], order=c(1,0,0))
  AR_net_predict <- forecast(AR_net, h =1)
  y_real =Data_uni[w_size + i]
  
  predicts_arnet <- c(predicts_arnet, AR_net_predict$mean)
  trues_arnet <- c(trues_arnet, y_real)
  e<-y_real -  AR_net_predict$mean
}



## Only with adaptive elastic net model
#y_predict = predict(aenet.fit, t(matrix(X[w_size + i,Xsis$Index[1:120]])))
#y_real = matrix(Y[w_size + i])

#predicts_anet <- c(predicts_anet,y_predict)
#trues_anet <-c(trues_anet, y_real)
#ee<-y_real - y_predict
errors <-c(errors,e)
}

```


2. Adaptive Elastic Net Model +  Pre -SIS procedure  (Expanding Window)

```{r}
library(glm2)
library('SISadaElasticNet',lib.loc = "SISadaElasticNet")
library( msaenet)
        
 # local R package https://github.com/zikiki/SISadaElasticNet
Data = us_inf_all

X = matrix(Data[,-1 ],nrow = dim(Data[,-1 ])[1])
Y = matrix(Data[,1],nrow = dim(Data[,-1 ])[1])

j= 48
w_size = j
n_windows = nrow(us_inf_all) - j 
# initialize the coeff_prior and # of variable inclusiong for model updating... 
predicts_arnet<-c()
trues_arnet<-c()
predicts_anet<-c()
trues_anet<-c()
errors <-c()
for (i in 1:n_windows) {

Xsis<-SIS.gaussian(X[1:(w_size + i - 1),], matrix(Y[1:(w_size + i - 1),1]),48)
aenet.fit = aenet(Xsis$Xs, matrix(Y[1:(w_size + i - 1),1]),
alphas = seq(0.2, 0.8, 0.02), seed = 10)
coef(aenet.fit) ## return the regressors' coefficients (the order of the regressors are ranked from the SIS.gaussian function)
nonzero_index = which(!coef(aenet.fit) == 0) ## return the index of the regressors in the ordered list

# Build an AR model that includes these regressors as well as the lagged term
if (length(nonzero_index) !=0){
  nonzero_index <-as.vector(nonzero_index)
  xregs <- X[1:(w_size + i - 1),Xsis$Index[1:48][nonzero_index]]
  newxregs <-t(matrix(X[w_size + i,Xsis$Index[1:48][nonzero_index]]))
  
  Data_uni = ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12)
  AR_net <-Arima(Data_uni[1:(w_size + i - 1)], order=c(1,0,0),xreg =xregs)
  AR_net_predict <- forecast(AR_net, xreg = newxregs , h =1)
  y_real =Data_uni[w_size + i]
  
  predicts_arnet <- c(predicts_arnet, AR_net_predict$mean)
  trues_arnet <- c(trues_arnet, y_real)
  e<-y_real -  AR_net_predict$mean
} else {
  Data_uni = ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12)
  AR_net <-Arima(Data_uni[1:(w_size + i - 1)], order=c(1,0,0))
  AR_net_predict <- forecast(AR_net, h =1)
  y_real =Data_uni[w_size + i]
  
  predicts_arnet <- c(predicts_arnet, AR_net_predict$mean)
  trues_arnet <- c(trues_arnet, y_real)
  e<-y_real -  AR_net_predict$mean
}



## Only with adaptive elastic net model
#y_predict = predict(aenet.fit, t(matrix(X[w_size + i,Xsis$Index[1:120]])))
#y_real = matrix(Y[w_size + i])

#predicts_anet <- c(predicts_anet,y_predict)
#trues_anet <-c(trues_anet, y_real)
#ee<-y_real - y_predict
errors <-c(errors,e)
}

```



Benchmark model: AR model 
```{r}
#index(us_inf_all)

pacf(ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12))
Data = ts(us_inf_all[,'inf'],start=c(2004, 3), end=c(2019, 1), frequency=12)
##AR(1) Mode 


predicts_ar<-c()
trues_ar<-c()
errors_ar <-c()
for (i in 1:n_windows) {
  
AR_model <- arima(Data[i:(w_size + i - 1)], order=c(1,0,0))
AR_predict <- predict(AR_model, 1)
y_real =Data[w_size + i]

predicts_ar <- c(predicts_ar, AR_predict$pred)
trues_ar <- c(trues_ar, y_real)
e<-y_real -  AR_predict$pred

}

#ts.plot(Data[1:48], AR_predict$pred, lty = c(1,3), col=c(5,2))

```


```{r}
## plot the predicts and true 

values = seq(from = as.Date("2008-03-01"), to = as.Date("2019-01-01"), by = 'month')
df <- data.frame(date = values, predicts = predicts_arnet, true = trues_arnet)
colnames(df) <-c('date','Adaptive Elastic Net Out-of-sample Forecasts', 'Inflation Changes (US)')

df <- melt(df ,  id.vars = 'date')
ggplot(data = df) + geom_line(aes(x = date, y = value, linetype = variable, color = variable))
```
```{r}
## plot the predicts_ar and true 

values = seq(from = as.Date("2008-03-01"), to = as.Date("2019-01-01"), by = 'month')
df <- data.frame(date = values, predicts = predicts_ar, true = trues_ar)
colnames(df) <-c('date','AR Model Out-of-sample Forecasts', 'Inflation Changes (US)')

df <- melt(df ,  id.vars = 'date')
ggplot(data = df) + geom_line(aes(x = date, y = value, linetype = variable, color = variable))
```

Point Forecast Performance: compare Adaptive Elastic Net and AR
```{r}

  mspe_aenet = trues_arnet - predicts_arnet
  mspe_ar = trues_ar - predicts_ar
  mspe_AENET = mean(na.omit(mspe_aenet)^2)
  mspe_AR = mean(na.omit(mspe_ar)^2)
  mspe_ratio = mspe_AENET/mspe_AR

  dm_re = dm.test(mspe_ar,mspe_aenet, "greater",h = 1)
  
```

compare Lasso and RW

```{r}
  mspe_random = trues_arnet - tail(Data[,'inf_lag'], n=n_windows)

  mspe = trues_arnet - predicts_arnet

  mspe_L = mean(na.omit(mspe)^2)
  mspe_RW = mean(na.omit(mspe_random)^2)
  mspe_ratio_LR = mspe_L/mspe_RW

  
  
  dm_LR = dm.test(as.matrix(mspe_random), mspe, "greater",h = 1)
  
```

calculate the success ratio
```{r}
forecasts <-data.frame(predicts_arnet, trues_arnet)
colnames(forecasts) <-c('Google_Forecasts','Realized')

forecasts$success <- ifelse(sign(forecasts$Google_Forecasts) == sign(forecasts$Realized), 1,0)
success_ratio = sum(forecasts$success)/nrow(forecasts)
forecasts$forecasted_direction <- sign(forecasts$Google_Forecasts)
forecasts$true_direction <- sign(forecasts$Realized)

forecasts$forecasted_direction <- ifelse(sign(forecasts$forecasted_direction) == 1, 1,0)


forecasts$true_direction <- ifelse(sign(forecasts$true_direction) == 1, 1,0)

```

Calculate R-squared 

```{r}

R_sq <- function(forecasts, real){
      R_sq <- 1 -(sum((real-forecasts)^2))/(sum((real-mean(real))^2))
      return(R_sq)
  }

PE_lasso <- R_sq(forecasts$Google_Forecasts, forecasts$Realized)
```




```{r}
forecasts$newey_west = forecasts$forecasted_direction * forecasts$Realized 
```


construct the autocovariance of Newey-LRV
```{r}

# autocovariance <-  as.numeric(NeweyWest(lm(forecasts$newey_west[1:110] ~ 1), adjust = TRUE))

nw <- function(y,qn){
  #input: y is a T*k vector and qn is the truncation lag
  #output: the newey west HAC covariance estimator
  #Formulas are from Hayashi
  T <- length(y)
  ybar <- rep(1,T) * ((sum(y))/T)
  dy <- y-ybar
  G0 <- t(dy) %*% dy/T
  for (j in 1:qn){
    gamma <- t(dy[(j+1):T]) %*% dy[1:T-j]/(T-1)
    G0 <- G0+(gamma+t(gamma))*(1-abs(j/qn))
  }
  return(as.numeric(G0))
}


N  = length(forecasts$newey_west-1)
test_statistics<- sqrt(N)*(mean(forecasts$newey_west))/sqrt(nw(y = forecasts$newey_west, qn = 1))


Directional_NW <- function(W){
  
  nw <- function(y,qn){
    #input: y is a T*k vector and qn is the truncation lag
    #output: the newey west HAC covariance estimator
    #Formulas are from Hayashi
    T <- length(y)
    ybar <- rep(1,T) * ((sum(y))/T)
    dy <- y-ybar
    G0 <- t(dy) %*% dy/T
    for (j in 1:qn){
      gamma <- t(dy[(j+1):T]) %*% dy[1:T-j]/(T-1)
      G0 <- G0+(gamma+t(gamma))*(1-abs(j/qn))
    }
    return(as.numeric(G0))
  }
  
  P <- length(W)
  varfroll.adj <- nw(W,1)
  CW <- sqrt(P)*(mean(W))/sqrt(varfroll.adj)
  pv <- 1-pnorm(CW,0,1)
  results=list(test=0,pvalue=0)
  results$test <- CW
  results$pvalue <- pv
  return(results)
}

```

```{r}
success_ratio
test_statistics
```



Form the forecast for ER

```{r}
In_sample <- Data$unem[1:48]
length(In_sample)
In_sample <-as.data.frame(In_sample)
uk_unem_reg <-rbind(as.matrix(In_sample), predicts)
path = file.path(data_dir, "uk_unem_reg.csv")
write.csv(uk_unem_reg, file = path)
```
